# -*- coding: utf-8 -*-
"""mock_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BcBfdNwVHUahOc8zwluxDBZ1gVKf6UpC

# Analysis of Voting Habits
"""

from google.colab import drive
drive.mount('/content/drive')

"""## Importing Libraries"""

import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import plotly.express as px
from sklearn.cluster import KMeans

"""## Importing the Dataset"""

file_path = "US_Swing_States.csv"
swing_df = pd.read_csv(file_path)
swing_df.head()

swing_df.info()

#dropping cloumns
new_swing_df = swing_df.drop(['token', 'submitdate', 'Mode','Q4_other','Q6_other','Q13Someone_5','Q13Someone_6','Q14_other','Q15_other','Q20','Q22_other','Q66'], axis=1)
new_swing_df.head()

#searching for missing values in each column
new_swing_df.isna().sum()
new_swing_df.info(verbose=True)
pd.set_option('display.max_rows', 76)

new_swing_df

output_file_path = "New_US_Swing_States.csv"
new_swing_df.to_csv(output_file_path, index=False)

#Loading data
file_path ="New_US_Swing_States.csv"
df_swing =pd.read_csv(file_path)
df_swing.head(10)

#List dataframe data types
df_swing.dtypes
df_swing.info(verbose=True)
pd.set_option('display.max_rows', 76)

# Drop null rows
df_swing = df_swing.dropna()

#find duplicate entries
print(f"Duplicates entries: {df_swing.duplicated().sum()}")

#List dataframe data types
df_swing.dtypes
df_swing.info(verbose=True)
pd.set_option('display.max_rows', 78)

# If we consider the Q13 column, it contains 7 categorical values which a particular voter can have
df_swing['Q13'].value_counts()



# Change 1 and 2 as 1: Predicted Trump Voters
# Change 3 and 4 as 2: Predicted Biden Voters
# Change Q13 response 5, 6,and 8 as 3

df_swing['encoded_Q13A'] = 0
df_swing.loc[df_swing['Q13'] == 1, 'encoded_Q13A'] = 1
df_swing.loc[df_swing['Q13'] == 2, 'encoded_Q13A'] = 1
df_swing.loc[df_swing['Q13'] == 3, 'encoded_Q13A'] = 2
df_swing.loc[df_swing['Q13'] == 4, 'encoded_Q13A'] = 2
df_swing.loc[df_swing['Q13'] == 5, 'encoded_Q13A'] = 3
df_swing.loc[df_swing['Q13'] == 6, 'encoded_Q13A'] = 3
df_swing.loc[df_swing['Q13'] == 8, 'encoded_Q13A'] = 3
df_swing.head()

df_swing['encoded_Q13A'].value_counts()

#To remove all rows where column 'encoded_Q13A' is > 2
df_swing = df_swing.drop(df_swing[df_swing.encoded_Q13A > 2].index)

df_swing['encoded_Q13A'].value_counts()

#To remove all rows where column 'encoded_Q13' is > 2
df_swing = df_swing.drop(df_swing[df_swing.encoded_Q13 > 2].index)

df_swing['encoded_Q13'].value_counts()

# print the label species(1, 2)
print(df_swing.encoded_Q13A)

# print data(feature)shape
print(df_swing.shape)

#dropping cloumns
new_swing_df = swing_df.drop(['Q13', 'encoded_Q13'], axis=1)
new_swing_df.head()

from sklearn import preprocessing
# Import train_test_split function
from sklearn.model_selection import train_test_split
#Import knearest neighbors Classifier model
from sklearn.neighbors import KNeighborsClassifier
#Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
# training a KNN model
from sklearn.neighbors import KNeighborsRegressor
# measuring RMSE score
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LogisticRegression

"""# Comparing Error Rate with the K Value"""

# separate train features and label
y = df_swing["encoded_Q13A"]
X = df_swing.drop(labels=["encoded_Q13A"], axis=1)

# split dataset into train and test data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y) # 80% training and 20% test

error = []

# Calculating error for K values between 1 and 40
for i in range(1, 40, 2):
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train, y_train)
    pred_i = knn.predict(X_test)
    error.append(np.mean(pred_i != y_test))

plt.figure(figsize=(12, 6))
plt.plot(range(1, 40, 2), error, color='red', linestyle='dashed', marker='*',
         markerfacecolor='blue', markersize=10)
plt.title('Error Rate K Value')
plt.xlabel('K Value')
plt.ylabel('Mean Error')

"""# Preprocessing

# Training and Predictions
"""

# separate train features and label Model Evaluation for k=3
y = df_swing["encoded_Q13A"]
X = df_swing.drop(labels=["encoded_Q13A"], axis=1)

# split dataset into train and test dat2
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y) # 80% training and 40% test

#Create KNN Classifier
knn = KNeighborsClassifier()

#Train the model using the training sets
knn.fit(X_train, y_train)

#Predict the response for test dataset
y_pred = knn.predict(X_test)

print("Accuracy is: ",metrics.accuracy_score(y_test, y_pred)*100, "%")

# separate train features and label Model Evaluation for k=3
y = df_swing["encoded_Q13A"]
X = df_swing.drop(labels=["encoded_Q13A"], axis=1)

# split dataset into train and test data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y) # 80% training and 20% test

#Create KNN Classifier
knn = KNeighborsClassifier(n_neighbors=3)

#Train the model using the training sets
knn.fit(X_train, y_train)

#Predict the response for test dataset
y_pred = knn.predict(X_test)

print("Accuracy is: ",metrics.accuracy_score(y_test, y_pred)*100, "%")

# separate train features and label Model Evaluation for k=5
y = df_swing["encoded_Q13A"]
X = df_swing.drop(labels=["encoded_Q13A"], axis=1)

# split dataset into train and test data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y) # 80% training and 20% test

#Create KNN Classifier
knn = KNeighborsClassifier(n_neighbors=5)

#Train the model using the training sets
knn.fit(X_train, y_train)

#Predict the response for test dataset
y_pred = knn.predict(X_test)

print("Accuracy is: ",metrics.accuracy_score(y_test, y_pred)*100, "%")

# separate train features and label Model Evaluation for k=7
y = df_swing["encoded_Q13A"]
X = df_swing.drop(labels=["encoded_Q13A"], axis=1)

# split dataset into train and test data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y) # 80% training and 20% test

#Create KNN Classifier
knn = KNeighborsClassifier(n_neighbors=7)

#Train the model using the training sets
knn.fit(X_train, y_train)

#Predict the response for test dataset
y_pred = knn.predict(X_test)

print("Accuracy is: ",metrics.accuracy_score(y_test, y_pred)*100, "%")

# separate train features and label Model Evaluation for k=9
y = df_swing["encoded_Q13A"]
X = df_swing.drop(labels=["encoded_Q13A"], axis=1)

# split dataset into train and test data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y) # 80% training and 20% test

#Create KNN Classifier
knn = KNeighborsClassifier(n_neighbors=9)

#Train the model using the training sets
knn.fit(X_train, y_train)

#Predict the response for test dataset
y_pred = knn.predict(X_test)

print("Accuracy is: ",metrics.accuracy_score(y_test, y_pred)*100, "%")

# separate train features and label Model Evaluation for k=11
y = df_swing["encoded_Q13A"]
X = df_swing.drop(labels=["encoded_Q13A"], axis=1)

# split dataset into train and test data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y) # 80% training and 20% test

#Create KNN Classifier
knn = KNeighborsClassifier(n_neighbors=11)

#Train the model using the training sets
knn.fit(X_train, y_train)

#Predict the response for test dataset
y_pred = knn.predict(X_test)

print("Accuracy is: ",metrics.accuracy_score(y_test, y_pred)*100, "%")

# separate train featuresA and label Model Evaluation for k=13
y = df_swing["encoded_Q13"]
X = df_swing.drop(labels=["encoded_Q13A"], axis=1)

# split dataset into train and test data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y) # 80% training and 20% test

#Create KNN Classifier
knn = KNeighborsClassifier(n_neighbors=13)

#Train the model using the training sets
knn.fit(X_train, y_train)

#Predict the response for test dataset
y_pred = knn.predict(X_test)

print("Accuracy is: ",metrics.accuracy_score(y_test, y_pred)*100, "%")

# separate train features and label Model Evaluation for k=15
y = df_swing["encoded_Q13A"]
X = df_swing.drop(labels=["encoded_Q13A"], axis=1)

# split dataset into train and test dat8
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y) # 80% training and 20% test

#Create KNN Classifier
knn = KNeighborsClassifier(n_neighbors=15)

#Train the model using the training sets
knn.fit(X_train, y_train)

#Predict the response for test dataset
y_pred = knn.predict(X_test)

print("Accuracy is: ",metrics.accuracy_score(y_test, y_pred)*100, "%")

# separate train features and label Model Evaluation for k=17
y = df_swing["encoded_Q13A"]
X = df_swing.drop(labels=["encoded_Q13A"], axis=1)

# split dataset into train and test data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y) # 80% training and 20% test

#Create KNN Classifier
knn = KNeighborsClassifier(n_neighbors=17)

# Train the model using the training sets
knn.fit(X_train, y_train)

#Predict the response for test dataset
y_pred = knn.predict(X_test)

print("Accuracy is: ",metrics.accuracy_score(y_test, y_pred)*100, "%")

## we had poor performance on this model, realize we need to scale the data
## run the model again
## How did you choose 7???

## should never be an even number so eliminate the event of a "tie" occuring


## Next Steps
## - Choose a value of K - "9" on unscaled data
## - Scale the data - keep 9
## - Look at diff values of K to see if we could better results

"""## Feature Importance"""

## Univariate Selection

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

# apply SelectKBest class to extract top 10 best features
bestfeatures = SelectKBest(score_func=chi2, k=10)
fit = bestfeatures.fit(X, y)
dfscores = pd.DataFrame(fit.scores_)
dfcolumns = pd.DataFrame(X.columns)

# concat two dataframes for better visualization 
featureScores = pd.concat([dfcolumns, dfscores], axis=1)
featureScores.columns = ['Specs', 'Score']  #naming the dataframe columns
print(featureScores.nlargest(76, 'Score'))  #print 10 best features

## Drop the following Columns less than 3

#66           Q68     2.927842
#4             Q4     2.547698
#14           Q14     1.947704
#69           Q71     1.384954
#50           Q51     1.043646
#7             Q7     1.021818
#60           Q61     0.937765
#71           Q73     0.688743
#72           Q74     0.553474
#68           Q70     0.364654
#17           Q17     0.278120
#70           Q72     0.157166
#65           Q67     0.001131
#1             Q1     0.000391
#8             Q8     0.000000

X.drop(labels=["Q68"], axis=1, inplace=True)
X.drop(labels=["Q4"], axis=1, inplace=True)
X.drop(labels=["Q14"], axis=1, inplace=True)
X.drop(labels=["Q71"], axis=1, inplace=True)
X.drop(labels=["Q51"], axis=1, inplace=True)
X.drop(labels=["Q7"], axis=1, inplace=True)
X.drop(labels=["Q61"], axis=1, inplace=True)
X.drop(labels=["Q73"], axis=1, inplace=True)
X.drop(labels=["Q74"], axis=1, inplace=True)
X.drop(labels=["Q70"], axis=1, inplace=True)
X.drop(labels=["Q17"], axis=1, inplace=True)
X.drop(labels=["Q72"], axis=1, inplace=True)
X.drop(labels=["Q67"], axis=1, inplace=True)
X.drop(labels=["Q1"], axis=1, inplace=True)
X.drop(labels=["Q8"], axis=1, inplace=True)
X.drop(labels=["Q13"], axis=1, inplace=True)
X.drop(labels=["encoded_Q13"], axis=1, inplace=True)

## Feature Importance

from sklearn.ensemble import ExtraTreesClassifier
import matplotlib.pyplot as plt
model = ExtraTreesClassifier()
model.fit(X,y)
print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers

#plot graph of feature importances for better visualization
feat_importances = pd.Series(model.feature_importances_, index=X.columns)
feat_importances.nlargest(10).plot(kind='barh')
plt.show()

#List dataframe data types
X.dtypes
X.info(verbose=True)
pd.set_option('display.max_rows', 76)

"""# Training and Predictions

## K-Nearest
"""

#Training and Predictions
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)

X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

# Running label Model Evaluation for k=9 as it was the best performing without unscaled data
classifier = KNeighborsClassifier(n_neighbors=9)
classifier.fit(X_train, y_train)

#Make predictions on our test data
y_pred = classifier.predict(X_test)
print("Accuracy is: ",metrics.accuracy_score(y_test, y_pred)*100, "%")

X_train[:5]

y_pred = classifier.predict(X_test)
results = pd.DataFrame({"Prediction": y_pred, "Actual": y_test}).reset_index(drop=True)
results.head(20)

y_pred

y_train

from sklearn.metrics import classification_report, confusion_matrix
# Calculating the confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Create a DataFrame from the confusion matrix.
cm_df = pd.DataFrame(
    cm, index=["Actual 1", "Actual 2"], columns=["Predicted 1", "Predicted 2"])

cm_df

print(classification_report(y_test, y_pred))

# The results show that:

# Out of 135 Trump loan applications (Actual 1), 113 were predicted to be Trump (Predicted 1), which we call true positives.
# Out of 135 Trump loan applications (Actual 1), 2 were predicted to be Biden (Predicted 2), which are considered false negatives.
# Out of 145 Biden loan applications (Actual 2), 1 were predicted to be Trump (Predicted 1) and are considered false positives.
# Out of 145 Biden loan applications (Actual 2), 144 were predicted to be Biden (Predicted 2) and are considered true negatives.

"""# LogisticRegression"""

import pandas as pd
import numpy as np
from sklearn import preprocessing
import matplotlib.pyplot as plt 
plt.rc("font", size=14)
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
import seaborn as sns
sns.set(style="white")
sns.set(style="whitegrid", color_codes=True)

y = df_swing["encoded_Q13A"]
X = df_swing.drop(labels=["encoded_Q13A"], axis=1)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,
   y, random_state=1, stratify=y)

from sklearn.linear_model import LogisticRegression
from sklearn import metrics

classifier = LogisticRegression(solver='lbfgs',
   max_iter=1000,
   random_state=1)

classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)

from sklearn.metrics import accuracy_score
print("Accuracy is: ",metrics.accuracy_score(y_test, y_pred)*100, "%")

"""# Evaluating the Algorithm"""

X_train.shape
y_train

y_pred

y_pred = classifier.predict(X_test)
results = pd.DataFrame({"Prediction": y_pred, "Actual": y_test}).reset_index(drop=True)
results.head(10)

from sklearn.metrics import confusion_matrix, classification_report
matrix = confusion_matrix(y_test, y_pred)
print(matrix)

report = classification_report(y_test, y_pred)
print(report)

